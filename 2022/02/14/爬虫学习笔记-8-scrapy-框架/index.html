<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>爬虫学习笔记(8)--scrapy 框架 |  一方天地</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      <canvas class="fireworks"></canvas>
      <style>
        .fireworks {
          position: fixed;
          left: 0;
          top: 0;
          z-index: 99999;
          pointer-events: none;
        }
      </style>
      
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-爬虫学习笔记-8-scrapy-框架"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  爬虫学习笔记(8)--scrapy 框架
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/02/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8-scrapy-%E6%A1%86%E6%9E%B6/" class="article-date">
  <time datetime="2022-02-14T07:37:40.000Z" itemprop="datePublished">2022-02-14</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python/">Python</a> / <a class="article-category-link" href="/categories/Python/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">5.1k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">22 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="1-scrapy-框架初识">1. scrapy 框架初识</h2>
<ul>
<li>
<p>什么是框架？</p>
<p>就是一个集成了很多功能并且具有很强通用性的一个项目模板。</p>
</li>
<li>
<p>如何学习框架？</p>
<p>专门学习框架封装的各种功能的详细用法。</p>
</li>
<li>
<p>什么是<code>scrapy</code>？</p>
<p>爬虫中封装好的一个明星框架。</p>
<p>功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式</p>
</li>
</ul>
<span id="more"></span>
<h2 id="2-scrapy-基本使用">2. scrapy 基本使用</h2>
<p>scrapy 框架的基本使用：</p>
<ul>
<li>环境的安装：
<ul>
<li>mac or linux：<code>pip install scrapy</code></li>
<li>windows:
<ul>
<li><code>pip install wheel</code></li>
<li>下载twisted，下载地址：<a target="_blank" rel="noopener" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></li>
<li>安装twisted：<code>pip install Twisted-20.3.0-cp39-cp39-win_amd64.whl</code></li>
<li><code>pip install pywin32</code></li>
<li><code>pip install scrapy</code></li>
<li>测试：在终端里录入scrapy指令，没有报错即表示安装成功！</li>
</ul>
</li>
</ul>
</li>
<li>创建一个工程：<code>scrapy startproject xxxPro</code></li>
<li><code>cd xxxPro</code></li>
<li>在spiders子目录中创建一个爬虫文件
<ul>
<li><code>scrapy genspider spiderName www.xxx.com</code></li>
</ul>
</li>
<li>执行工程：
<ul>
<li><code>scrapy crawl spiderName</code></li>
</ul>
</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###firstBlood__first</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    <span class="comment"># 爬虫文件的名称：就是爬虫源文件的一个唯一标识</span></span><br><span class="line">    name = <span class="string">&#x27;first&#x27;</span></span><br><span class="line">    <span class="comment"># 允许的域名：用来限定start_urls列表中哪些url可以进行请求发送</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.baidu.com&#x27;]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.baidu.com/&#x27;</span>, <span class="string">&#x27;https://www.sogou.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用作于数据解析：response参数表示的就是请求成功后对应的响应对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure></div>
<h2 id="3-scrapy-数据解析操作">3. scrapy 数据解析操作</h2>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QiubaiSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;qiubai&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># 解析作者的名称+段子的内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，当时列表元素一定是Selector类型的对象</span></span><br><span class="line">            <span class="comment"># extract可以将Selector对象中data参数存储的字符串提取出来</span></span><br><span class="line">            author = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># 列表调用了extract之后。则表示将列表中每一个Selector对象中data对应的字符串提取了出来</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span//text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(author,content)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></div>
<h2 id="4-基于终端指令的持久化存储">4. 基于终端指令的持久化存储</h2>
<p>scrapy 持久化存储：</p>
<ul>
<li>基于终端指令：
<ul>
<li>要求：只可以将 parse 方法的返回值存储到本地的文本文件中</li>
<li>注意：持久化存储对应的文本文件类型只可以为：json、jsonlines、jl、csv、xml、marshal、pickle</li>
<li>指令：<code>scrapy crawl xxx -o filePath</code></li>
<li>好处：简洁高效便捷</li>
<li>缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）</li>
</ul>
</li>
</ul>
<h2 id="5-基于管道持久化存储操作">5. 基于管道持久化存储操作</h2>
<p>基于管道：</p>
<ul>
<li>编码流程：
<ul>
<li>数据解析</li>
<li>在item类中定义相关的属性</li>
<li>将解析的数据封装到item类型的对象</li>
<li>将item类型的对象提交给管道进行持久化存储的操作</li>
<li>在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作</li>
<li>在配置文件中开启管道</li>
</ul>
</li>
<li>好处：
<ul>
<li>通用性强。</li>
</ul>
</li>
</ul>
<p>面试题：将爬取到的数据一份存储到本地，一份存储到数据库，如何实现？</p>
<ul>
<li>管道文件中一个管道类对应的是将数据存储到一种平台</li>
<li>爬虫文件提交的 item 只会给管道文件中第一个被执行的管道类接收</li>
<li><code>process_item</code>中的<code>return item</code>表示将 item 传递给下一个即将被执行的管道类</li>
</ul>
<h2 id="6-全站数据爬取">6. 全站数据爬取</h2>
<p>基于 spider 的全站数据爬取：就是将网站中某板块下的全部页码对应的页面数据进行爬取。</p>
<ul>
<li>爬取：校花网明星写真的名称</li>
<li>实现方式：
<ul>
<li>将所有页面的<code>url</code>添加到<code>start_urls</code>列表（不推荐）</li>
<li>自行手动进行请求发送（推荐）</li>
</ul>
</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;------------校花网xiaohua.py----------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;xiaohua&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.521609.com/tuku/mxxz/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#生成一个通用的url模板(不可变)</span></span><br><span class="line">    url = <span class="string">&#x27;http://www.521609.com/tuku/mxxz/index_%d.html&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;/html/body/div[4]/div[3]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            img_name = li.xpath(<span class="string">&#x27;./a/p/text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="built_in">print</span>(img_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt;= <span class="number">28</span>:</span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url%self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line">            <span class="comment">#手动请求发送:callback回调函数是专门用作于数据解析</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_url,callback=self.parse)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------校花网pipelines.py--------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaohuaproPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;----------------校花网settings.py部分代码---------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&#x27;</span></span><br></pre></td></tr></table></figure></div>
<h2 id="7-五大核心组件">7. 五大核心组件</h2>
<p>五大核心组件：</p>
<p><img src="https://cdn.jsdelivr.net/gh/zyzhang827/filesimage@main/images/2022/01/05/23354211d021735432927c98f97c955d-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6-4b62c0.png" alt="核心组件"></p>
<ul>
<li>Spiders：
<ul>
<li>产生 URL，对 URL 进行手动发送</li>
<li>进行数据解析</li>
</ul>
</li>
<li>引擎（Scrapy Engine）：
<ul>
<li>数据流处理</li>
<li>触发事务</li>
</ul>
</li>
<li>调度器（Scheduler）：
<ul>
<li>过滤器去重</li>
<li>去重后的请求对象压到队列中</li>
</ul>
</li>
<li>下载器（Downloader）：
<ul>
<li>负责获取页面数据并提供给引擎，而后提供给 Spider</li>
</ul>
</li>
<li>项目管道（Item Pipeline）：
<ul>
<li>负责处理爬虫从网页中抽取的实体，页面被爬虫解析所需的数据存入 item 后，将被发送到管道，经过特定的次序处理数据，最后存入本地文件或者数据库。</li>
</ul>
</li>
</ul>
<h2 id="8-请求传参">8. 请求传参</h2>
<ul>
<li>使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）</li>
<li>需求：爬取 boss 的岗位名称和岗位描述</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### 我尝试着并未有啥结果.......等大佬</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bossPro.items <span class="keyword">import</span> BossproItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BossSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;boss&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.zhipin.com/c100010000/?page=1&amp;ka=page-1&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    url = <span class="string">&#x27;https://www.zhipin.com/c100010000/?page=%d&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 回调函数接收item</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        job_desc = response.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div[3]/div/div[2]/div[2]/div[1]/div//text()&#x27;</span>).extract()</span><br><span class="line">        job_desc = <span class="string">&#x27;&#x27;</span>.join(job_desc)</span><br><span class="line">        <span class="built_in">print</span>(job_desc)</span><br><span class="line">        item[<span class="string">&#x27;job_desc&#x27;</span>] = job_desc</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析首页中的岗位名称</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div/div[2]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            item = BossproItem()</span><br><span class="line"></span><br><span class="line">            job_name = li.xpath(<span class="string">&#x27;.//div/div[1]/div[1]/div/div[1]/span[1]/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;job_name&#x27;</span>] = job_name</span><br><span class="line">            <span class="built_in">print</span>(job_name)</span><br><span class="line">            detail_url = <span class="string">&#x27;https://www.zhipin.com&#x27;</span> + li.xpath(<span class="string">&#x27;.//div/div[1]/div[1]/div/div[1]/span[1]/a/@href&#x27;</span>).extract_first()</span><br><span class="line">            <span class="comment"># 对详情页发请求获取详情页的页面源码数据</span></span><br><span class="line">            <span class="comment"># 手动请求的发送</span></span><br><span class="line">            <span class="comment"># 请求传参：meta=&#123;&#125;，可以将meta字典传递给请求对应的回调函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(detail_url,callback=self.parse_detail,meta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分页操作</span></span><br><span class="line">        <span class="keyword">if</span> self.page_num &lt;= <span class="number">5</span>:</span><br><span class="line">            new_url = <span class="built_in">format</span>(self.url%self.page_num)</span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_url,callback=self.parse)</span><br></pre></td></tr></table></figure></div>
<h2 id="9-scrapy-图片爬取">9. scrapy 图片爬取</h2>
<p>图片数据爬取之 ImagesPipline：</p>
<ul>
<li>
<p>基于 scrapy 爬取字符串类型的数据和爬取图片类型的数据区别？</p>
<ul>
<li>字符串：只需要基于 xpath 进行解析且提交管道进行持久化存储</li>
<li>图片：xpath 解析出图片的 src 属性值，单独的对图片地址发起请求获取二进制类型的数据</li>
</ul>
</li>
<li>
<p>ImagesPipeline：</p>
<ul>
<li>只需要将 img 的 src 的属性值进行解析，提交到管道，管道就会对图片的 src 进行请求发送获取图片的二进制类型的数据，且还会帮我们进行持久化存储。</li>
</ul>
</li>
<li>
<p>需求：爬取站长素材的高清图片</p>
</li>
<li>
<p>使用流程：</p>
<ul>
<li>数据解析（图片的地址）</li>
<li>将存储图片地址的 item 提交到指定的管道类</li>
<li>在管道文件中自己定制一个基于 ImagesPipeLine 的一个管道类
<ul>
<li><code>get_media_request( )</code></li>
<li><code>file_path</code></li>
<li><code>item_completed</code></li>
</ul>
</li>
<li>在配置文件中操作
<ul>
<li>指定图片存储目录：<code>IMAGES_STORE = './imgs_ZYZhang'</code></li>
<li>指定开启的管道：自定制的管道类</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;----------------爬取站长素材高清图片  img.py-----------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> imgsPro.items <span class="keyword">import</span> ImgsproItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;img&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://sc.chinaz.com/tupian/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@id=&quot;container&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment">#注意：使用伪属性 src2</span></span><br><span class="line">            src = <span class="string">&#x27;https:&#x27;</span> + div.xpath(<span class="string">&#x27;./div/a/img/@src2&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            item = ImgsproItem()</span><br><span class="line">            item[<span class="string">&#x27;src&#x27;</span>] = src</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;----------------------爬取站长素材高清图片  pipelines.py---------------------------&#x27;&#x27;&#x27;</span>            </span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># class ImgsproPipeline(object):</span></span><br><span class="line"><span class="comment">#     def process_item(self, item, spider):</span></span><br><span class="line"><span class="comment">#         return item</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">imgsPileLine</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以根据图片地址进行图片数据的请求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(item[<span class="string">&#x27;src&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定图片存储的路径</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span></span><br><span class="line">        imgName = request.url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> imgName</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self, results, item, info</span>):</span></span><br><span class="line">        <span class="keyword">return</span> item <span class="comment"># 返回给下一个即将被执行的管道类</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------------------------爬取站长素材高清图片  items.py-----------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgsproItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    src = scrapy.Field()</span><br><span class="line">    <span class="comment"># pass</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;------------------------------爬取站长素材高清图片 setting.py部分代码-------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 指定图片存储的目录</span></span><br><span class="line">IMAGES_STORE = <span class="string">&#x27;./imgs_ZYZhang&#x27;</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;imgsPro.pipelines.imgsPileLine&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br></pre></td></tr></table></figure></div>
<h2 id="10-中间件">10. 中间件</h2>
<ul>
<li>下载中间件：
<ul>
<li>位置：引擎和下载器之间</li>
<li>作用：批量拦截到整个工程中所有的请求和响应</li>
<li>拦截请求：
<ul>
<li>UA 伪装：<code>process_request</code></li>
<li>代理 IP：<code>process_exception:return request</code></li>
</ul>
</li>
<li>拦截响应：
<ul>
<li>篡改响应数据，响应对象</li>
<li>网易新闻爬取</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="11-网易新闻">11. 网易新闻</h2>
<p>需求：爬取网易新闻的新闻数据（标题和内容）</p>
<ul>
<li>通过网易新闻的首页解析出几大板块对应的详情页的 url（经验证，无动态加载）</li>
<li>每个板块点击后，其中的新闻标题都是动态加载出来的（动态加载）</li>
<li>通过解析出每一条新闻详情页的 url，获取详情页的页面源码，解析出新闻内容</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------------网易新闻  wangyi.py------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> wangyiPro.items <span class="keyword">import</span> WangyiproItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WangyiSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;wangyi&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.cccom&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://news.163.com/&#x27;</span>]</span><br><span class="line">    models_urls = []  <span class="comment">#存储五个板块对应详情页的url</span></span><br><span class="line">    <span class="comment">#解析五大板块对应详情页的url</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#实例化一个浏览器对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.bro = webdriver.Chrome(executable_path=<span class="string">&#x27;F:\PythonProjects\爬虫\动态加载数据处理\chromedriver.exe&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;index2016_wrap&quot;]/div[1]/div[2]/div[2]/div[2]/div[2]/div/ul/li&#x27;</span>)</span><br><span class="line">        alist = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> alist:</span><br><span class="line">            model_url = li_list[index].xpath(<span class="string">&#x27;./a/@href&#x27;</span>).extract_first()</span><br><span class="line">            self.models_urls.append(model_url)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#依次对每一个板块对应的页面进行请求</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.models_urls:      <span class="comment">#对每一个板块的url进行请求发送</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#每一个板块对应的新闻标题相关的内容都是动态加载</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_model</span>(<span class="params">self,response</span>):</span>    <span class="comment">#解析每一个板块页面中对应新闻的标题和新闻详情页的url</span></span><br><span class="line">        <span class="comment"># response.xpath()</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;/html/body/div/div[3]/div[4]/div[1]/div/div/ul/li/div/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div/div[1]/h3/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            new_detail_url = div.xpath(<span class="string">&#x27;./div/div[1]/h3/a/@href&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            item = WangyiproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line"></span><br><span class="line">            <span class="comment">#对新闻详情页的url发起请求</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_detail_url, callback=self.parse_detail, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">self,response</span>):</span>       <span class="comment"># 解析新闻内容</span></span><br><span class="line">        content = response.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div[2]//text()&#x27;</span>).extract()</span><br><span class="line">        content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closed</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.bro.quit()</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------------网易新闻  pipelines.py-----------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WangyiproPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------------网易新闻  middlewares.py-------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your spider middleware</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WangyiproDownloaderMiddleware</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy框架 acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="comment"># Called for each request that goes through the downloader</span></span><br><span class="line">        <span class="comment"># middleware.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this request</span></span><br><span class="line">        <span class="comment"># - or return a Response object</span></span><br><span class="line">        <span class="comment"># - or return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest: process_exception() methods of</span></span><br><span class="line">        <span class="comment">#   installed downloader middleware will be called</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过该方法拦截五大板块对应的响应对象，进行篡改，使其满足需求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span>    <span class="comment">#spider爬虫对象</span></span><br><span class="line">        bro = spider.bro  <span class="comment">#获取了在爬虫类中定义的浏览器对象</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#挑选出指定的响应对象进行篡改</span></span><br><span class="line">        <span class="comment">#    通过url指定request</span></span><br><span class="line">        <span class="comment">#    通过request指定response</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> spider.models_urls:</span><br><span class="line">            bro.get(request.url)   <span class="comment">#五个板块对应的url进行请求</span></span><br><span class="line">            sleep(<span class="number">3</span>)</span><br><span class="line">            page_text = bro.page_source  <span class="comment">#包含了动态加载的新闻数据</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#response #五大板块对应的响应对象</span></span><br><span class="line">            <span class="comment">#针对定位到的这些response进行篡改</span></span><br><span class="line">            <span class="comment">#实例化一个新的响应对象（符合需求：包含动态加载出的新闻数据），替代原来旧的响应对象</span></span><br><span class="line">            <span class="comment">#如何获取动态加载出的新闻数据？</span></span><br><span class="line">                <span class="comment">#基于selenium便捷的获取动态加载数据</span></span><br><span class="line">            new_response = HtmlResponse(url=request.url, body=page_text, encoding=<span class="string">&#x27;utf-8&#x27;</span>, request=request)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> new_response</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#response #其他请求对应的响应对象</span></span><br><span class="line">            <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span></span><br><span class="line">        <span class="comment"># Called when a download handler or a process_request()</span></span><br><span class="line">        <span class="comment"># (from other downloader middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this exception</span></span><br><span class="line">        <span class="comment"># - return a Response object: stops process_exception() chain</span></span><br><span class="line">        <span class="comment"># - return a Request object: stops process_exception() chain</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-----------------------------网易新闻 setting.py部分代码---------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#USER_AGENT = &#x27;wangyiPro (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;wangyiPro.middlewares.WangyiproDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;wangyiPro.pipelines.WangyiproPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;ERROR&#x27;</span></span><br></pre></td></tr></table></figure></div>
<h2 id="12-CrawlSpider-的全站数据爬取">12. CrawlSpider 的全站数据爬取</h2>
<p>CrawlSpider：基于 Spider 的一个子类</p>
<ul>
<li>全站数据爬取的方式
<ul>
<li>基于 Spider：手动请求发送</li>
<li>基于 CrawlSpider</li>
</ul>
</li>
<li>CrawlSpider的使用：
<ul>
<li>创建一个工程</li>
<li>cd  XXX</li>
<li>创建爬虫文件（CrawlSpider）
<ul>
<li><code>scrapy  genspider  -t  crawl  xxx   www.xxxx.com</code></li>
<li>链接提取器（LinkExtractor）：根据指定规则（allow=“正则”）进行指定链接的提取</li>
<li>规则解析器（Rule）：将链接提取器提取到的链接进行指定规则（callback）的解析操作</li>
</ul>
</li>
</ul>
</li>
<li>需求：爬取阳光热线网站中的编号，新闻标题，新闻内容，标号
<ul>
<li>分析：爬取的数据没有在同一张页面中</li>
<li>
<ol>
<li>可以使用链接提取器提取所有的页码链接</li>
<li>让链接提取器提取所有的问政详情页链接</li>
</ol>
</li>
</ul>
</li>
</ul>
<div class="highlight-wrap" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------------阳光问政    sun.py---------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;网站页面源码跟视频课有改动，建议follow先改False爬一下，不然容易被封IP，有兴趣的可以改改，搞个代理啥的再爬&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> sunPro.items <span class="keyword">import</span> SunproItem, DetailItem</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需求：爬取阳光热线网站中的编号，新闻标题，新闻内容，标号</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SunSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;sun&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#链接提取器：根据指定规则（allow=&quot;正则&quot;）进行指定链接的提取</span></span><br><span class="line">    link = LinkExtractor(allow=<span class="string">r&#x27;id=1&amp;page=\d+&#x27;</span>)</span><br><span class="line">    link_detail = LinkExtractor(allow=<span class="string">r&#x27;index\?id=\d+&#x27;</span>)</span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment">#规则解析器：将链接提取器提取到的链接进行指定规则（callback）的解析操作</span></span><br><span class="line">        Rule(link, callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">False</span>),</span><br><span class="line">        <span class="comment">#follow=True：可以将链接提取器 继续作用到 链接提取器提取到的链接 所对应的页面中</span></span><br><span class="line">        Rule(link_detail, callback=<span class="string">&#x27;parse_detail&#x27;</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#http://wz.sun0769.com/political/politics/index?id=490505</span></span><br><span class="line">    <span class="comment">#http://wz.sun0769.com/political/politics/index?id=490504</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析新闻编号和新闻的标题</span></span><br><span class="line">    <span class="comment"># 如下两个解析方法中是不可以实现请求传参！</span></span><br><span class="line">    <span class="comment"># 无法将两个解析方法解析的数据存储到同一个item中，可以依次存储到两个item中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># 注意：xpath表达式中不可以出现tbody标签</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;/html//div[2]/div[3]/ul[2]/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            new_num = li.xpath(<span class="string">&#x27;./span[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">            new_title = li.xpath(<span class="string">&#x27;./span[3]/a/text()&#x27;</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            item = SunproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = new_title</span><br><span class="line">            item[<span class="string">&#x27;new_num&#x27;</span>] = new_num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析新闻内容和新闻编号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">self,response</span>):</span></span><br><span class="line">        new_id = response.xpath(<span class="string">&#x27;/html//div[3]/div[2]/div[2]/div[1]/span[4]/text()&#x27;</span>).extract_first().strip().replace(<span class="string">&quot;\r\n&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        new_content = response.xpath(<span class="string">&#x27;/html//div[3]/div[2]/div[2]/div[2]/pre/text()&#x27;</span>).extract()</span><br><span class="line">        new_content = <span class="string">&#x27;&#x27;</span>.join(new_content)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(new_id,new_content)</span></span><br><span class="line">        item = DetailItem()</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = new_content</span><br><span class="line">        item[<span class="string">&#x27;new_id&#x27;</span>] = new_id</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">        </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------------pipelines.py------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SunproPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="comment"># 如何判定item的类型</span></span><br><span class="line">        <span class="comment"># 将数据写入数据库时，如何保证数据的一致性</span></span><br><span class="line">        <span class="keyword">if</span> item.__class__.__name__ == <span class="string">&#x27;DetailItem&#x27;</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;new_id&#x27;</span>],item[<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;new_num&#x27;</span>],item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------------------items.py----------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SunproItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    new_num = scrapy.Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DetailItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    new_id = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></div>
<h2 id="13-分布式概述及搭建">13. 分布式概述及搭建</h2>
<p>分布式爬虫：</p>
<ul>
<li>概念：我们需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取。</li>
<li>作用：提升爬取数据的效率</li>
</ul>
<p>如何实现分布式？</p>
<ul>
<li>安装一个 scrapy-redis 的组件</li>
<li>原生的 scrapy 是不可以实现分布式爬虫的，必须要让 crapy-redis 组件一起实现分布式爬虫。</li>
</ul>
<p>为什么原生的 scrapy 不可以实现分布式？</p>
<ul>
<li>调度器不可以被分布式机群共享</li>
<li>管道不可以被分布式机群共享</li>
</ul>
<p>scrapy-redis 组件作用：</p>
<ul>
<li>可以给原生的 scrapy 框架提供可以被共享的<em>管道</em>和<em>调度器</em>。</li>
</ul>
<p>scrapy-redis 实现流程：</p>
<ul>
<li>
<p>创建一个工程</p>
</li>
<li>
<p>创建一个基于 CrawlSpider 的爬虫文件</p>
</li>
<li>
<p>修改当前的爬虫文件：</p>
<ul>
<li>导包：<code>from scrapy_redis.spiders  import  RedisCrawlSpider</code></li>
<li>将 start_urls 和 allowed_domains 进行注释</li>
<li>添加一个新属性：<code>redis_key = '   '</code> 可以被共享的调度器队列的名称</li>
<li>编写数据解析相关的操作</li>
<li>将当前爬虫类的父类修改成 RedisCrawlSpider</li>
</ul>
</li>
<li>
<p>修改配置文件 settings</p>
<ul>
<li>
<p>指定使用可以被共享的管道：</p>
</li>
<li>
<p><code>ITEM_PIPELINES = &#123;'scrapy_redis.pipelines.RedisPipeline': 400 &#125;</code></p>
</li>
<li>
<p>指定调度器：</p>
</li>
<li>
<p>增加了一个去重容器类的配置，作用是用 Redis 的 set 集合来存储请求的指纹数据，从而实现请求去重的持久化</p>
<p><code>DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</code></p>
<p>使用scrapy-redis组件自己的调度器</p>
<p><code>SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</code></p>
<p>配置调度器是否要持久化，也就是当爬虫结束了，要不要清空 Redis 中请求队列和去重指纹的 set。如果是 True，就表示要持久化存储，就不清数据，否则清空数据</p>
<p><code>SCHEDULER_PERSIST = True</code></p>
</li>
<li>
<p>指定redis服务器</p>
</li>
</ul>
</li>
<li>
<p>redis 相关操作配置：</p>
<ul>
<li>配置 redis 的配置文件：
<ul>
<li>linux 或者 mac：<code>redis.conf</code></li>
<li>windows：<code>redis.windows.conf</code></li>
<li>打开配置文件修改：
<ul>
<li>将<code>bind 127.0.0.1</code>进行注释或删除</li>
<li>关闭保护模式：<code>protected-mode yes</code>改为no</li>
</ul>
</li>
</ul>
</li>
<li>结合着配置文件开启 redis 服务
<ul>
<li>redis-server 配置文件</li>
<li>启动客户端：redis-cli</li>
</ul>
</li>
</ul>
</li>
<li>
<p>执行工程：</p>
<ul>
<li><code>scrapy  runspider  xxx.py</code></li>
</ul>
</li>
<li>
<p>向调度器的队列中放入一个起始的 url：</p>
<ul>
<li>调度器的队列在 redis 的客户端中</li>
<li><code>lpush  xxx  www.xxx.com</code></li>
</ul>
</li>
<li>
<p>爬取到的数据存储在了 redis 的 <code>proName:items</code> 这个数据结构中</p>
</li>
</ul>
<h2 id="14-增量式爬虫">14. 增量式爬虫</h2>
<ul>
<li>概念：监测网站数据更新的情况，只会爬取网站最新更新出来的数据。</li>
<li>分析：
<ul>
<li>指定一个起始 url</li>
<li>基于 CrawlSpider 获取其他页码链接</li>
<li>基于 Rule 将其他页码链接进行请求</li>
<li>从每一个页码对应的页面源码中解析出每一个电影详情页的 URL</li>
<li>核心：检测电影详情页的 url 之前有没有请求过
<ul>
<li>将爬取过的电影详情页的url存储</li>
<li>存储到 redis 的 set 数据结构</li>
</ul>
</li>
<li>对详情页的 url 发起请求，然后解析出电影的名称和简介</li>
<li>进行持久化存储</li>
</ul>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://example.com/2022/02/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8-scrapy-%E6%A1%86%E6%9E%B6/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/02/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-9-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B-%E8%A1%A5%E5%85%85/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            爬虫学习笔记(9)--异步编程 补充
          
        </div>
      </a>
    
    
      <a href="/2022/02/14/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7-%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">爬虫学习笔记(7)--动态加载数据处理</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "n4pHsUlbm3Bkd829iMBXvzAw-gzGzoHsz",
    app_key: "oJ4qdGrUrguFexAiIRkAzPlb",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2022
        <i class="ri-heart-fill heart_icon"></i> ZYZhang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/zhangguorong.ico" alt="一方天地"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script>

<script src="/js/clickBoom1.js"></script>
 
<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
  <!-- 雪花特效 -->

  <!--单击显示文字-->
  
  <script type="text/javascript" src="/js/show_text.js"></script>

</body>

</html>